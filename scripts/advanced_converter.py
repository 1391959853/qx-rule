#!/usr/bin/env python3
"""
Quantumult X è§„åˆ™é«˜çº§è½¬æ¢ç³»ç»Ÿ
æ”¯æŒåˆ†æµè§„åˆ™å’Œé‡å†™è§„åˆ™ä¸­çš„DOMAINè½¬æ¢
"""

import os
import re
import sys
import json
import requests
import configparser
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Set, Optional
from urllib.parse import urlparse, urljoin
import logging
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('conversion.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AdvancedRuleConverter:
    def __init__(self, github_token: str = None):
        """
        åˆå§‹åŒ–é«˜çº§è§„åˆ™è½¬æ¢å™¨
        
        Args:
            github_token: GitHub Personal Access Token
        """
        self.github_token = github_token
        self.session = requests.Session()
        
        if github_token:
            self.session.headers.update({
                'Authorization': f'token {github_token}',
                'User-Agent': 'QX-Rule-Converter/2.0'
            })
        else:
            self.session.headers.update({
                'User-Agent': 'QX-Rule-Converter/2.0'
            })
        
        # åˆ†æµè§„åˆ™è½¬æ¢æ˜ å°„ï¼ˆDOMAIN -> HOSTï¼‰
        self.filter_rule_conversions = {
            # DOMAIN -> HOST
            r'(?i)^DOMAIN,': 'HOST,',
            r'(?i)^DOMAIN-SUFFIX,': 'HOST-SUFFIX,',
            r'(?i)^DOMAIN-KEYWORD,': 'HOST-KEYWORD,',
            # ç¡®ä¿å…¶ä»–è§„åˆ™ç±»å‹å¤§å†™
            r'(?i)^IP-CIDR,': 'IP-CIDR,',
            r'(?i)^IP-CIDR6,': 'IP-CIDR6,',
            r'(?i)^GEOIP,': 'GEOIP,',
            r'(?i)^URL-REGEX,': 'URL-REGEX,',
            r'(?i)^PROCESS-NAME,': 'PROCESS-NAME,',
        }
        
        # é‡å†™è§„åˆ™è½¬æ¢æ˜ å°„
        self.rewrite_rule_conversions = {
            # é‡å†™è§„åˆ™ä¸­çš„DOMAINåŒ¹é…
            r'(?i)^DOMAIN,([^,]+),([^,]+)(?:,([^,]+))?$': self._convert_rewrite_domain,
            r'(?i)^DOMAIN-SUFFIX,([^,]+),([^,]+)(?:,([^,]+))?$': self._convert_rewrite_domain_suffix,
            r'(?i)^DOMAIN-KEYWORD,([^,]+),([^,]+)(?:,([^,]+))?$': self._convert_rewrite_domain_keyword,
            # URLæ­£åˆ™é‡å†™è§„åˆ™ï¼ˆä¿æŒä¸å˜ï¼‰
            r'^\^https?://': None,  # æ ‡è®°ä¸ºURLé‡å†™è§„åˆ™
        }
        
        # ç­–ç•¥æ˜ å°„
        self.policy_mappings = {
            'DIRECT': 'DIRECT',
            'PROXY': 'PROXY',
            'REJECT': 'REJECT',
            'å¹¿å‘Šæ‹¦æˆª': 'REJECT',
            'å»å¹¿å‘Š': 'REJECT',
            'å…¨çƒç›´è¿': 'DIRECT',
            'å›½å†…ç›´è¿': 'DIRECT',
            'å›½å¤–ä»£ç†': 'PROXY',
            'å›½å¤–ç½‘ç«™': 'PROXY',
            'no-resolve': 'no-resolve',
        }
        
        # å·²å¤„ç†çš„è§„åˆ™å“ˆå¸Œé›†åˆ
        self.processed_hashes = {
            'filter': set(),
            'rewrite': set()
        }
        
        # è½¬æ¢ç»Ÿè®¡
        self.stats = {
            'total_sources': 0,
            'successful_downloads': 0,
            'failed_downloads': [],
            'filter_rules_count': 0,
            'rewrite_rules_count': 0,
            'domain_conversions': 0,
            'source_repos': []
        }
    
    def parse_rule_ini(self, ini_path: str) -> Dict:
        """
        è§£ærule.iniæ–‡ä»¶
        
        Args:
            ini_path: rule.iniæ–‡ä»¶è·¯å¾„
            
        Returns:
            è§£æåçš„é…ç½®å­—å…¸
        """
        config = configparser.ConfigParser()
        
        # æ”¯æŒå¤šè¡Œurls
        try:
            config.read(ini_path, encoding='utf-8')
        except Exception as e:
            logger.error(f"è§£æINIæ–‡ä»¶å¤±è´¥: {str(e)}")
            # å°è¯•æ‰‹åŠ¨è§£æ
            return self._manual_parse_ini(ini_path)
        
        result = {
            'filter_remote': [],
            'rewrite_remote': [],
            'other_sections': {}
        }
        
        # å¤„ç†[filter_remote]
        if 'filter_remote' in config:
            urls_text = config['filter_remote'].get('urls', '')
            # å¤„ç†å¤šè¡ŒURLï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰
            urls = []
            for line in urls_text.split('\n'):
                line = line.strip()
                if line and not line.startswith('#'):
                    # ä¸€è¡Œå¯èƒ½æœ‰å¤šä¸ªURLï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
                    for url in line.split():
                        if url and not url.startswith('#'):
                            urls.append(url.strip())
            result['filter_remote'] = urls
        
        # å¤„ç†[rewrite_remote]
        if 'rewrite_remote' in config:
            urls_text = config['rewrite_remote'].get('urls', '')
            urls = []
            for line in urls_text.split('\n'):
                line = line.strip()
                if line and not line.startswith('#'):
                    for url in line.split():
                        if url and not url.startswith('#'):
                            urls.append(url.strip())
            result['rewrite_remote'] = urls
        
        # å¤„ç†å…¶ä»–èŠ‚
        for section in config.sections():
            if section not in ['filter_remote', 'rewrite_remote']:
                result['other_sections'][section] = dict(config[section])
        
        logger.info(f"è§£æåˆ° {len(result['filter_remote'])} ä¸ªåˆ†æµé“¾æ¥å’Œ {len(result['rewrite_remote'])} ä¸ªé‡å†™é“¾æ¥")
        return result
    
    def _manual_parse_ini(self, ini_path: str) -> Dict:
        """
        æ‰‹åŠ¨è§£æINIæ–‡ä»¶ï¼ˆå½“configparserè§£æå¤±è´¥æ—¶ï¼‰
        
        Args:
            ini_path: iniæ–‡ä»¶è·¯å¾„
            
        Returns:
            è§£æåçš„é…ç½®å­—å…¸
        """
        result = {
            'filter_remote': [],
            'rewrite_remote': [],
            'other_sections': {}
        }
        
        current_section = None
        
        with open(ini_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if not line or line.startswith('#'):
                    continue
                
                # æ£€æŸ¥èŠ‚æ ‡é¢˜
                if line.startswith('[') and line.endswith(']'):
                    current_section = line[1:-1]
                    continue
                
                # å¤„ç†URL
                if current_section == 'filter_remote':
                    if line.startswith('urls') or '=' in line:
                        # æå–URLéƒ¨åˆ†
                        if '=' in line:
                            urls_part = line.split('=', 1)[1].strip()
                        else:
                            urls_part = line
                        
                        # åˆ†å‰²URL
                        for url in urls_part.split():
                            url = url.strip()
                            if url and not url.startswith('#'):
                                result['filter_remote'].append(url)
                
                elif current_section == 'rewrite_remote':
                    if line.startswith('urls') or '=' in line:
                        if '=' in line:
                            urls_part = line.split('=', 1)[1].strip()
                        else:
                            urls_part = line
                        
                        for url in urls_part.split():
                            url = url.strip()
                            if url and not url.startswith('#'):
                                result['rewrite_remote'].append(url)
        
        logger.info(f"æ‰‹åŠ¨è§£æåˆ° {len(result['filter_remote'])} ä¸ªåˆ†æµé“¾æ¥å’Œ {len(result['rewrite_remote'])} ä¸ªé‡å†™é“¾æ¥")
        return result
    
    def download_content(self, url: str) -> Optional[Tuple[str, str, str]]:
        """
        ä¸‹è½½å†…å®¹
        
        Args:
            url: ä¸‹è½½é“¾æ¥
            
        Returns:
            (å†…å®¹ç±»å‹, å†…å®¹, å®é™…URL) æˆ– None
        """
        try:
            logger.info(f"ä¸‹è½½: {url}")
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                content = response.text
                
                # åˆ¤æ–­å†…å®¹ç±»å‹
                content_type = self._detect_content_type(content, url)
                
                # è®°å½•åŸå§‹ä»“åº“ä¿¡æ¯
                repo_info = self._extract_repo_info(url)
                if repo_info not in self.stats['source_repos']:
                    self.stats['source_repos'].append(repo_info)
                
                logger.info(f"ä¸‹è½½æˆåŠŸ: {len(content)} å­—èŠ‚ ({content_type})")
                return content_type, content, response.url
            else:
                logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {response.status_code}")
                self.stats['failed_downloads'].append({
                    'url': url,
                    'error': f"HTTP {response.status_code}"
                })
                
        except Exception as e:
            logger.error(f"ä¸‹è½½å¼‚å¸¸ {url}: {str(e)}")
            self.stats['failed_downloads'].append({
                'url': url,
                'error': str(e)
            })
        
        return None
    
    def _detect_content_type(self, content: str, url: str) -> str:
        """
        æ£€æµ‹å†…å®¹ç±»å‹
        
        Args:
            content: å†…å®¹
            url: åŸå§‹URL
            
        Returns:
            å†…å®¹ç±»å‹: 'filter', 'rewrite', 'mixed', 'unknown'
        """
        content_lower = content.lower()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å†™è§„åˆ™
        rewrite_indicators = [
            '^http',  # URLé‡å†™
            ' url ',  # é‡å†™è§„åˆ™æ ¼å¼
            'reject',  # æ‹’ç»è§„åˆ™
            '302', '307',  # é‡å®šå‘
            '[rewrite]',  # é‡å†™èŠ‚
        ]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†æµè§„åˆ™
        filter_indicators = [
            'domain,',
            'host,',
            'ip-cidr,',
            'geoip,',
            'url-regex,',
            '[filter]',
            '[rule]',
        ]
        
        rewrite_count = sum(1 for indicator in rewrite_indicators if indicator in content_lower)
        filter_count = sum(1 for indicator in filter_indicators if indicator in content_lower)
        
        # æ ¹æ®URLåç¼€åˆ¤æ–­
        url_lower = url.lower()
        if any(ext in url_lower for ext in ['.list', '.txt', '.rules']):
            return 'filter'
        elif any(ext in url_lower for ext in ['.conf', '.rewrite', '.js']):
            return 'rewrite'
        
        # æ ¹æ®å†…å®¹åˆ¤æ–­
        if rewrite_count > filter_count:
            return 'rewrite'
        elif filter_count > rewrite_count:
            return 'filter'
        elif rewrite_count > 0 and filter_count > 0:
            return 'mixed'
        else:
            return 'unknown'
    
    def _extract_repo_info(self, url: str) -> Dict:
        """
        ä»URLæå–ä»“åº“ä¿¡æ¯
        
        Args:
            url: åŸå§‹URL
            
        Returns:
            ä»“åº“ä¿¡æ¯å­—å…¸
        """
        try:
            # GitHubé“¾æ¥
            github_match = re.search(r'github\.com/([^/]+)/([^/]+)', url)
            if github_match:
                owner = github_match.group(1)
                repo = github_match.group(2)
                full_repo = f"{owner}/{repo}"
                
                # è·å–ä»“åº“æè¿°ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                repo_desc = self._get_github_repo_desc(full_repo)
                
                return {
                    'type': 'github',
                    'owner': owner,
                    'repo': repo,
                    'full_repo': full_repo,
                    'url': f"https://github.com/{full_repo}",
                    'description': repo_desc,
                    'raw_url': url
                }
            
            # GitLabé“¾æ¥
            gitlab_match = re.search(r'gitlab\.com/([^/]+)/([^/]+)', url)
            if gitlab_match:
                owner = gitlab_match.group(1)
                repo = gitlab_match.group(2)
                full_repo = f"{owner}/{repo}"
                
                return {
                    'type': 'gitlab',
                    'owner': owner,
                    'repo': repo,
                    'full_repo': full_repo,
                    'url': f"https://gitlab.com/{full_repo}",
                    'raw_url': url
                }
            
            # å…¶ä»–é“¾æ¥
            return {
                'type': 'other',
                'url': url,
                'raw_url': url
            }
            
        except Exception as e:
            logger.warning(f"æå–ä»“åº“ä¿¡æ¯å¤±è´¥ {url}: {str(e)}")
        
        return {
            'type': 'unknown',
            'url': url,
            'raw_url': url
        }
    
    def _get_github_repo_desc(self, repo: str) -> str:
        """
        è·å–GitHubä»“åº“æè¿°
        
        Args:
            repo: ä»“åº“åï¼ˆowner/repoï¼‰
            
        Returns:
            ä»“åº“æè¿°æˆ–ç©ºå­—ç¬¦ä¸²
        """
        try:
            if self.github_token:
                api_url = f"https://api.github.com/repos/{repo}"
                headers = {'Authorization': f'token {self.github_token}'}
                
                response = self.session.get(api_url, headers=headers, timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    return data.get('description', '')
        except:
            pass
        return ''
    
    def convert_filter_rule(self, line: str) -> str:
        """
        è½¬æ¢åˆ†æµè§„åˆ™è¡Œ
        
        Args:
            line: åŸå§‹è§„åˆ™è¡Œ
            
        Returns:
            è½¬æ¢åçš„è§„åˆ™è¡Œ
        """
        original_line = line.strip()
        
        # ç©ºè¡Œæˆ–æ³¨é‡Šè¡Œ
        if not original_line or original_line.startswith('#'):
            return original_line
        
        # è·³è¿‡èŠ‚æ ‡é¢˜
        if original_line.startswith('[') and original_line.endswith(']'):
            return original_line
        
        # è·³è¿‡hostnameè¡Œ
        if original_line.lower().startswith('hostname'):
            return original_line
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºDOMAINç›¸å…³è§„åˆ™
        converted_line = original_line
        
        # åº”ç”¨DOMAIN -> HOSTè½¬æ¢
        domain_converted = False
        for pattern, replacement in self.filter_rule_conversions.items():
            if re.match(pattern, converted_line, re.IGNORECASE):
                converted_line = re.sub(pattern, replacement, converted_line, flags=re.IGNORECASE)
                if 'DOMAIN' in pattern.upper():
                    domain_converted = True
                break
        
        # å¦‚æœæ˜¯DOMAINè½¬æ¢ï¼Œæ›´æ–°ç»Ÿè®¡
        if domain_converted:
            self.stats['domain_conversions'] += 1
        
        # å¤„ç†ç­–ç•¥æ˜ å°„
        parts = converted_line.split(',')
        if len(parts) >= 3:
            policy = parts[-1].strip()
            mapped_policy = self.policy_mappings.get(policy.upper(), policy.upper())
            parts[-1] = mapped_policy
            converted_line = ','.join(parts)
        
        # ç¡®ä¿è§„åˆ™ç±»å‹å¤§å†™
        if ',' in converted_line:
            rule_type = converted_line.split(',')[0]
            if rule_type.isalpha():
                converted_line = rule_type.upper() + converted_line[len(rule_type):]
        
        return converted_line
    
    def convert_rewrite_rule(self, line: str) -> str:
        """
        è½¬æ¢é‡å†™è§„åˆ™è¡Œ
        
        Args:
            line: åŸå§‹è§„åˆ™è¡Œ
            
        Returns:
            è½¬æ¢åçš„è§„åˆ™è¡Œ
        """
        original_line = line.strip()
        
        # ç©ºè¡Œæˆ–æ³¨é‡Šè¡Œ
        if not original_line or original_line.startswith('#'):
            return original_line
        
        # è·³è¿‡èŠ‚æ ‡é¢˜
        if original_line.startswith('[') and original_line.endswith(']'):
            return original_line
        
        # è·³è¿‡hostnameè¡Œ
        if original_line.lower().startswith('hostname'):
            return original_line
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯DOMAINç±»å‹çš„é‡å†™è§„åˆ™
        for pattern, converter in self.rewrite_rule_conversions.items():
            if converter is None:
                # åªæ˜¯æ ‡è®°ï¼Œä¸éœ€è¦è½¬æ¢
                if re.match(pattern, original_line):
                    return original_line
            else:
                match = re.match(pattern, original_line, re.IGNORECASE)
                if match:
                    try:
                        converted_line = converter(match.groups())
                        if converted_line:
                            self.stats['domain_conversions'] += 1
                            return converted_line
                    except Exception as e:
                        logger.warning(f"è½¬æ¢é‡å†™è§„åˆ™å¤±è´¥: {original_line}, é”™è¯¯: {str(e)}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯URLé‡å†™è§„åˆ™ï¼ˆä¿æŒåŸæ ·ï¼‰
        if ' url ' in original_line:
            return original_line
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å…¶ä»–ç±»å‹çš„é‡å†™è§„åˆ™
        if original_line.startswith(('http', '^', 'hostname', 'mitm')):
            return original_line
        
        # å…¶ä»–æƒ…å†µï¼Œå°è¯•åº”ç”¨DOMAINè½¬æ¢
        converted_line = original_line
        for pattern, replacement in self.filter_rule_conversions.items():
            if 'DOMAIN' in pattern and re.match(pattern, converted_line, re.IGNORECASE):
                converted_line = re.sub(pattern, replacement, converted_line, flags=re.IGNORECASE)
                self.stats['domain_conversions'] += 1
                break
        
        return converted_line
    
    def _convert_rewrite_domain(self, groups: Tuple) -> str:
        """è½¬æ¢é‡å†™è§„åˆ™ä¸­çš„DOMAIN"""
        domain, action, *extra = groups
        result = f"HOST,{domain},{action.upper()}"
        if extra and extra[0]:
            result += f",{extra[0]}"
        return result
    
    def _convert_rewrite_domain_suffix(self, groups: Tuple) -> str:
        """è½¬æ¢é‡å†™è§„åˆ™ä¸­çš„DOMAIN-SUFFIX"""
        suffix, action, *extra = groups
        result = f"HOST-SUFFIX,{suffix},{action.upper()}"
        if extra and extra[0]:
            result += f",{extra[0]}"
        return result
    
    def _convert_rewrite_domain_keyword(self, groups: Tuple) -> str:
        """è½¬æ¢é‡å†™è§„åˆ™ä¸­çš„DOMAIN-KEYWORD"""
        keyword, action, *extra = groups
        result = f"HOST-KEYWORD,{keyword},{action.upper()}"
        if extra and extra[0]:
            result += f",{extra[0]}"
        return result
    
    def process_filter_content(self, content: str, source_url: str = "") -> List[str]:
        """
        å¤„ç†åˆ†æµè§„åˆ™å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            source_url: æºURL
            
        Returns:
            å¤„ç†åçš„è§„åˆ™åˆ—è¡¨
        """
        lines = content.split('\n')
        processed_lines = []
        source_comment = f"# Source: {source_url}" if source_url else ""
        
        for line in lines:
            line = line.rstrip()
            
            if not line:
                processed_lines.append("")
                continue
            
            # å¤„ç†è§„åˆ™è¡Œ
            converted_line = self.convert_filter_rule(line)
            
            # ç”Ÿæˆå“ˆå¸Œç”¨äºå»é‡
            rule_hash = hashlib.md5(converted_line.encode('utf-8')).hexdigest()
            
            if rule_hash not in self.processed_hashes['filter']:
                self.processed_hashes['filter'].add(rule_hash)
                processed_lines.append(converted_line)
                self.stats['filter_rules_count'] += 1
            else:
                # è§„åˆ™é‡å¤ï¼Œæ·»åŠ æ³¨é‡Š
                if not converted_line.startswith('#') and converted_line:
                    processed_lines.append(f"# {converted_line}  # [é‡å¤è§„åˆ™å·²è·³è¿‡]")
                else:
                    processed_lines.append(converted_line)
        
        # æ·»åŠ æ¥æºæ³¨é‡Š
        if source_comment and processed_lines:
            for i, line in enumerate(processed_lines):
                if line and not line.startswith('#'):
                    processed_lines.insert(i, source_comment)
                    processed_lines.insert(i + 1, "")
                    break
            else:
                processed_lines.insert(0, source_comment)
                processed_lines.insert(1, "")
        
        return processed_lines
    
    def process_rewrite_content(self, content: str, source_url: str = "") -> List[str]:
        """
        å¤„ç†é‡å†™è§„åˆ™å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            source_url: æºURL
            
        Returns:
            å¤„ç†åçš„è§„åˆ™åˆ—è¡¨
        """
        lines = content.split('\n')
        processed_lines = []
        source_comment = f"# Source: {source_url}" if source_url else ""
        
        for line in lines:
            line = line.rstrip()
            
            if not line:
                processed_lines.append("")
                continue
            
            # å¤„ç†è§„åˆ™è¡Œ
            converted_line = self.convert_rewrite_rule(line)
            
            # ç”Ÿæˆå“ˆå¸Œç”¨äºå»é‡
            rule_hash = hashlib.md5(converted_line.encode('utf-8')).hexdigest()
            
            if rule_hash not in self.processed_hashes['rewrite']:
                self.processed_hashes['rewrite'].add(rule_hash)
                processed_lines.append(converted_line)
                self.stats['rewrite_rules_count'] += 1
            else:
                # è§„åˆ™é‡å¤ï¼Œæ·»åŠ æ³¨é‡Š
                if not converted_line.startswith('#') and converted_line:
                    processed_lines.append(f"# {converted_line}  # [é‡å¤è§„åˆ™å·²è·³è¿‡]")
                else:
                    processed_lines.append(converted_line)
        
        # æ·»åŠ æ¥æºæ³¨é‡Š
        if source_comment and processed_lines:
            for i, line in enumerate(processed_lines):
                if line and not line.startswith('#'):
                    processed_lines.insert(i, source_comment)
                    processed_lines.insert(i + 1, "")
                    break
            else:
                processed_lines.insert(0, source_comment)
                processed_lines.insert(1, "")
        
        return processed_lines
    
    def process_mixed_content(self, content: str, source_url: str = "") -> Dict[str, List[str]]:
        """
        å¤„ç†æ··åˆå†…å®¹ï¼ˆåŒ…å«åˆ†æµå’Œé‡å†™è§„åˆ™ï¼‰
        
        Args:
            content: åŸå§‹å†…å®¹
            source_url: æºURL
            
        Returns:
            å¤„ç†åçš„è§„åˆ™å­—å…¸ {'filter': [], 'rewrite': []}
        """
        result = {
            'filter': [],
            'rewrite': []
        }
        
        lines = content.split('\n')
        current_section = None
        buffer = []
        
        for line in lines:
            line = line.rstrip()
            
            # æ£€æŸ¥èŠ‚æ ‡é¢˜
            if line.startswith('[') and line.endswith(']'):
                # å¤„ç†ä¹‹å‰çš„ç¼“å†²åŒº
                if buffer and current_section:
                    if current_section in ['filter', 'rules', 'filter_remote']:
                        result['filter'].extend(self.process_filter_content('\n'.join(buffer), source_url))
                        result['filter'].append("")
                    elif current_section in ['rewrite', 'rewrite_remote', 'rewrite_local']:
                        result['rewrite'].extend(self.process_rewrite_content('\n'.join(buffer), source_url))
                        result['rewrite'].append("")
                
                # æ›´æ–°å½“å‰èŠ‚
                section_name = line[1:-1].lower()
                if 'filter' in section_name or 'rules' in section_name:
                    current_section = 'filter'
                elif 'rewrite' in section_name:
                    current_section = 'rewrite'
                else:
                    current_section = None
                
                buffer = [line]
                continue
            
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            buffer.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªç¼“å†²åŒº
        if buffer and current_section:
            if current_section == 'filter':
                result['filter'].extend(self.process_filter_content('\n'.join(buffer), source_url))
            elif current_section == 'rewrite':
                result['rewrite'].extend(self.process_rewrite_content('\n'.join(buffer), source_url))
        
        return result
    
    def process_all_urls(self, ini_path: str, output_dir: str = "rules") -> Dict:
        """
        å¤„ç†æ‰€æœ‰URL
        
        Args:
            ini_path: rule.iniæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        # è§£æINIæ–‡ä»¶
        config = self.parse_rule_ini(ini_path)
        
        # å‡†å¤‡è¾“å‡º
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è¾“å‡ºç¼“å†²åŒº
        filter_output = []
        rewrite_output = []
        
        # æ·»åŠ æ–‡ä»¶å¤´
        filter_header = self._generate_file_header("Quantumult X åˆ†æµè§„åˆ™", ini_path)
        rewrite_header = self._generate_file_header("Quantumult X é‡å†™è§„åˆ™", ini_path)
        
        filter_output.extend(filter_header)
        rewrite_output.extend(rewrite_header)
        
        # å¤„ç†åˆ†æµè§„åˆ™é“¾æ¥
        logger.info(f"å¼€å§‹å¤„ç†åˆ†æµè§„åˆ™ ({len(config['filter_remote'])} ä¸ªé“¾æ¥)")
        for url in config['filter_remote']:
            result = self.download_content(url)
            if result:
                content_type, content, actual_url = result
                
                if content_type in ['filter', 'unknown']:
                    rules = self.process_filter_content(content, actual_url)
                    filter_output.extend(rules)
                    filter_output.append("")
                elif content_type == 'rewrite':
                    rules = self.process_rewrite_content(content, actual_url)
                    rewrite_output.extend(rules)
                    rewrite_output.append("")
                elif content_type == 'mixed':
                    mixed_result = self.process_mixed_content(content, actual_url)
                    filter_output.extend(mixed_result['filter'])
                    rewrite_output.extend(mixed_result['rewrite'])
                
                self.stats['successful_downloads'] += 1
            
            self.stats['total_sources'] += 1
        
        # å¤„ç†é‡å†™è§„åˆ™é“¾æ¥
        logger.info(f"å¼€å§‹å¤„ç†é‡å†™è§„åˆ™ ({len(config['rewrite_remote'])} ä¸ªé“¾æ¥)")
        for url in config['rewrite_remote']:
            result = self.download_content(url)
            if result:
                content_type, content, actual_url = result
                
                if content_type in ['rewrite', 'unknown']:
                    rules = self.process_rewrite_content(content, actual_url)
                    rewrite_output.extend(rules)
                    rewrite_output.append("")
                elif content_type == 'filter':
                    rules = self.process_filter_content(content, actual_url)
                    filter_output.extend(rules)
                    filter_output.append("")
                elif content_type == 'mixed':
                    mixed_result = self.process_mixed_content(content, actual_url)
                    filter_output.extend(mixed_result['filter'])
                    rewrite_output.extend(mixed_result['rewrite'])
                
                self.stats['successful_downloads'] += 1
            
            self.stats['total_sources'] += 1
        
        # ä¿å­˜åˆ†æµè§„åˆ™
        filter_file = output_path / "filter_rules.conf"
        with open(filter_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(filter_output))
        
        # ä¿å­˜é‡å†™è§„åˆ™
        rewrite_file = output_path / "rewrite_rules.conf"
        with open(rewrite_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(rewrite_output))
        
        # æ·»åŠ åŸä½œè€…ä»“åº“é“¾æ¥
        self._add_author_links(filter_file, rewrite_file, config)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = self._generate_stats()
        
        logger.info(f"å¤„ç†å®Œæˆï¼")
        logger.info(f"åˆ†æµè§„åˆ™: {self.stats['filter_rules_count']} æ¡")
        logger.info(f"é‡å†™è§„åˆ™: {self.stats['rewrite_rules_count']} æ¡")
        logger.info(f"DOMAINè½¬æ¢: {self.stats['domain_conversions']} æ¬¡")
        
        return stats
    
    def _generate_file_header(self, title: str, ini_path: str) -> List[str]:
        """
        ç”Ÿæˆæ–‡ä»¶å¤´
        
        Args:
            title: æ–‡ä»¶æ ‡é¢˜
            ini_path: iniæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶å¤´è¡Œåˆ—è¡¨
        """
        return [
            "# " + "=" * 50,
            f"# {title}",
            "# " + "=" * 50,
            f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"# é…ç½®æ–‡ä»¶: {ini_path}",
            "# è¯´æ˜: æ‰€æœ‰DOMAINè§„åˆ™å·²ç»Ÿä¸€è½¬æ¢ä¸ºHOSTè§„åˆ™",
            "# æ³¨æ„: å®Œå…¨ç›¸åŒçš„è§„åˆ™å·²è‡ªåŠ¨å»é‡",
            "# " + "=" * 50,
            ""
        ]
    
    def _add_author_links(self, filter_file: Path, rewrite_file: Path, config: Dict):
        """
        æ·»åŠ åŸä½œè€…ä»“åº“é“¾æ¥
        
        Args:
            filter_file: åˆ†æµè§„åˆ™æ–‡ä»¶
            rewrite_file: é‡å†™è§„åˆ™æ–‡ä»¶
            config: é…ç½®ä¿¡æ¯
        """
        # æ”¶é›†æ‰€æœ‰ä»“åº“é“¾æ¥
        all_repos = []
        
        # ä»åˆ†æµé“¾æ¥æå–
        for url in config['filter_remote']:
            repo_info = self._extract_repo_info(url)
            if repo_info not in all_repos:
                all_repos.append(repo_info)
        
        # ä»é‡å†™é“¾æ¥æå–
        for url in config['rewrite_remote']:
            repo_info = self._extract_repo_info(url)
            if repo_info not in all_repos:
                all_repos.append(repo_info)
        
        # ç”Ÿæˆä»“åº“é“¾æ¥éƒ¨åˆ†
        author_section = [
            "",
            "# " + "=" * 50,
            "# åŸä½œè€…ä»“åº“é“¾æ¥ (ç‚¹å‡»å³å¯å‰å¾€)",
            "# " + "=" * 50,
            ""
        ]
        
        for i, repo in enumerate(all_repos, 1):
            if repo['type'] == 'github':
                repo_name = repo.get('full_repo', repo.get('repo', 'GitHubä»“åº“'))
                repo_desc = repo.get('description', '')
                desc_text = f" - {repo_desc}" if repo_desc else ""
                
                author_section.append(f"# {i}. [{repo_name}]({repo['url']}){desc_text}")
                author_section.append(f"#    è§„åˆ™é“¾æ¥: {repo['raw_url']}")
            elif repo['type'] == 'gitlab':
                author_section.append(f"# {i}. GitLab: {repo['url']}")
                author_section.append(f"#    è§„åˆ™é“¾æ¥: {repo['raw_url']}")
            else:
                author_section.append(f"# {i}. è§„åˆ™é“¾æ¥: {repo['raw_url']}")
            author_section.append("")
        
        author_section.append("# " + "=" * 50)
        author_section.append("# æ„Ÿè°¢æ‰€æœ‰è§„åˆ™ä½œè€…çš„è´¡çŒ®ï¼")
        author_section.append("# " + "=" * 50)
        
        # æ·»åŠ åˆ°æ–‡ä»¶
        author_text = '\n'.join(author_section)
        
        # æ·»åŠ åˆ°åˆ†æµè§„åˆ™æ–‡ä»¶
        with open(filter_file, 'a', encoding='utf-8') as f:
            f.write('\n' + author_text)
        
        # æ·»åŠ åˆ°é‡å†™è§„åˆ™æ–‡ä»¶
        with open(rewrite_file, 'a', encoding='utf-8') as f:
            f.write('\n' + author_text)
    
    def _generate_stats(self) -> Dict:
        """
        ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = self.stats.copy()
        
        # è®¡ç®—æˆåŠŸç‡
        if stats['total_sources'] > 0:
            stats['success_rate'] = (stats['successful_downloads'] / stats['total_sources']) * 100
        else:
            stats['success_rate'] = 0
        
        # æ ¼å¼åŒ–æ—¶é—´
        stats['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        stats['output_files'] = ['filter_rules.conf', 'rewrite_rules.conf']
        
        return stats
    
    def generate_readme(self, stats: Dict, readme_path: str = "README.md"):
        """
        ç”Ÿæˆç¾è§‚çš„README
        
        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯
            readme_path: READMEæ–‡ä»¶è·¯å¾„
        """
        try:
            # è¯»å–ç°æœ‰READMEæˆ–åˆ›å»ºæ–°å†…å®¹
            if Path(readme_path).exists():
                with open(readme_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            else:
                content = "# Quantumult X è§„åˆ™è‡ªåŠ¨è½¬æ¢ä»“åº“\n\n"
            
            # ç”Ÿæˆæ›´æ–°æ—¥å¿—éƒ¨åˆ†
            update_section = self._generate_update_section(stats)
            
            # æŸ¥æ‰¾æˆ–åˆ›å»ºæ›´æ–°æ—¥å¿—éƒ¨åˆ†
            pattern = r'(<!-- UPDATE_LOG_START -->[\s\S]*?<!-- UPDATE_LOG_END -->)'
            
            if re.search(pattern, content):
                # æ›¿æ¢ç°æœ‰æ—¥å¿—
                content = re.sub(pattern, update_section, content)
            else:
                # åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ 
                content = content.replace("# Quantumult X è§„åˆ™è‡ªåŠ¨è½¬æ¢ä»“åº“\n\n", 
                                        "# Quantumult X è§„åˆ™è‡ªåŠ¨è½¬æ¢ä»“åº“\n\n" + update_section + "\n\n")
            
            # ä¿å­˜README
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info(f"READMEå·²æ›´æ–°: {readme_path}")
            
        except Exception as e:
            logger.error(f"ç”ŸæˆREADMEå¤±è´¥: {str(e)}")
    
    def _generate_update_section(self, stats: Dict) -> str:
        """
        ç”Ÿæˆæ›´æ–°æ—¥å¿—éƒ¨åˆ†
        
        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            Markdownæ ¼å¼çš„æ›´æ–°æ—¥å¿—
        """
        # ç”Ÿæˆå¾½ç« 
        badges = [
            f"![æœ€æ–°æ›´æ–°](https://img.shields.io/badge/æ›´æ–°-{stats['timestamp'].replace(' ', '--')}-blue)",
            f"![åˆ†æµè§„åˆ™](https://img.shields.io/badge/åˆ†æµ-{stats['filter_rules_count']}æ¡-green)",
            f"![é‡å†™è§„åˆ™](https://img.shields.io/badge/é‡å†™-{stats['rewrite_rules_count']}æ¡-orange)",
            f"![DOMAINè½¬æ¢](https://img.shields.io/badge/DOMAINè½¬æ¢-{stats['domain_conversions']}æ¬¡-yellow)",
            f"![æˆåŠŸç‡](https://img.shields.io/badge/æˆåŠŸç‡-{stats['success_rate']:.1f}%25-brightgreen)"
        ]
        
        if stats['failed_downloads']:
            badges.append(f"![å¤±è´¥é“¾æ¥](https://img.shields.io/badge/å¤±è´¥-{len(stats['failed_downloads'])}ä¸ª-red)")
        
        # ç”Ÿæˆè§„åˆ™æ¥æºåˆ—è¡¨
        repo_table = []
        for repo in stats['source_repos']:
            if repo['type'] == 'github':
                repo_name = repo.get('full_repo', repo.get('repo', 'GitHubä»“åº“'))
                repo_link = f"[{repo_name}]({repo['url']})"
                repo_desc = repo.get('description', '')
                repo_table.append(f"| {repo_link} | {repo_desc} | GitHub |")
            elif repo['type'] == 'gitlab':
                repo_table.append(f"| [{repo.get('full_repo', 'GitLabä»“åº“')}]({repo['url']}) | - | GitLab |")
            else:
                repo_table.append(f"| {repo['url']} | - | å…¶ä»– |")
        
        # ç”Ÿæˆå¤±è´¥åˆ—è¡¨
        failed_list = []
        if stats['failed_downloads']:
            for failed in stats['failed_downloads']:
                failed_list.append(f"- `{failed['url'][:50]}...` - {failed['error']}")
        
        return f"""<!-- UPDATE_LOG_START -->
## ğŸ“‹ è‡ªåŠ¨æ›´æ–°æ—¥å¿—

### ğŸš€ æœ€æ–°æ›´æ–° ({stats['timestamp']})

{' '.join(badges)}

### ğŸ“Š è½¬æ¢ç»Ÿè®¡

| é¡¹ç›® | æ•°é‡/çŠ¶æ€ |
|------|-----------|
| æ€»é“¾æ¥æ•° | {stats['total_sources']} |
| æˆåŠŸä¸‹è½½ | {stats['successful_downloads']} |
| å¤±è´¥ä¸‹è½½ | {len(stats['failed_downloads'])} |
| åˆ†æµè§„åˆ™æ•° | {stats['filter_rules_count']} |
| é‡å†™è§„åˆ™æ•° | {stats['rewrite_rules_count']} |
| DOMAINè½¬æ¢æ¬¡æ•° | {stats['domain_conversions']} |
| è§„åˆ™é‡å¤è¿‡æ»¤ | å·²å¯ç”¨ |
| è½¬æ¢æˆåŠŸç‡ | {stats['success_rate']:.1f}% |

### ğŸ“ è¾“å‡ºæ–‡ä»¶

| æ–‡ä»¶ç±»å‹ | æ–‡ä»¶å | è§„åˆ™æ•°é‡ | è¯´æ˜ |
|----------|--------|----------|------|
| åˆ†æµè§„åˆ™ | `rules/filter_rules.conf` | {stats['filter_rules_count']} | æ‰€æœ‰åˆ†æµè§„åˆ™ï¼ŒDOMAINå·²è½¬ä¸ºHOST |
| é‡å†™è§„åˆ™ | `rules/rewrite_rules.conf` | {stats['rewrite_rules_count']} | æ‰€æœ‰é‡å†™è§„åˆ™ï¼ŒDOMAINå·²è½¬ä¸ºHOST |

### ğŸ”— è§„åˆ™æ¥æºä»“åº“

| ä»“åº“ | æè¿° | ç±»å‹ |
|------|------|------|
{chr(10).join(repo_table)}

{f"### âŒ ä¸‹è½½å¤±è´¥åˆ—è¡¨{f"{chr(10)}{chr(10).join(failed_list)}" if failed_list else ""}

### ğŸ”„ è½¬æ¢è§„åˆ™è¯´æ˜

1. **DOMAINè½¬æ¢**:
   - `DOMAIN` â†’ `HOST`
   - `DOMAIN-SUFFIX` â†’ `HOST-SUFFIX`
   - `DOMAIN-KEYWORD` â†’ `HOST-KEYWORD`
   
2. **è§„åˆ™å¤„ç†**:
   - åˆ†æµè§„åˆ™å’Œé‡å†™è§„åˆ™åˆ†å¼€å¤„ç†
   - å®Œå…¨ç›¸åŒçš„è§„åˆ™è‡ªåŠ¨å»é‡
   - è§„åˆ™ç±»å‹ç»Ÿä¸€ä¸ºå¤§å†™
   - ç­–ç•¥åç§°æ ‡å‡†åŒ–

3. **å…¼å®¹æ€§**:
   - æ”¯æŒæ–°æ—§ç‰ˆQuantumult Xè§„åˆ™æ ¼å¼
   - è‡ªåŠ¨æ£€æµ‹è§„åˆ™ç±»å‹
   - æ™ºèƒ½åˆå¹¶æ··åˆå†…å®¹

### âš™ï¸ ä½¿ç”¨æ–¹æ³•

1. **Quantumult Xé…ç½®**:
   ```ini
   [filter_remote]
   https://raw.githubusercontent.com/ä½ çš„ç”¨æˆ·å/ä»“åº“/main/rules/filter_rules.conf
   
   [rewrite_remote]
   https://raw.githubusercontent.com/ä½ çš„ç”¨æˆ·å/ä»“åº“/main/rules/rewrite_rules.conf